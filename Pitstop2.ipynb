{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Pitstop2.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKBjhq8-TlIu"
      },
      "source": [
        "# Functions for Car Suspension Simulation\n",
        "*cleaned from cartsim_data.py*\n",
        "\n",
        "PS: Next stage of this work will be cleaning up this python script for a better structure and clear explanation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YV07HtF4RJf"
      },
      "source": [
        "#define _FILE_SPECS \"-rw-r--r-- 1 chris 8007 Mar 25 12:33 cartsim_data.py\"\n",
        "#define _MAGIC_NUMBER 1147484068\n",
        "import numpy as np\n",
        "import math, os,   sys,  time\n",
        "from time import gmtime, strftime\n",
        "# from logger import logger\n",
        "from datetime import date, datetime, timezone\n",
        "import statistics as st\n",
        "import random as rn\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import random as rnn\n",
        " \n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "#equations\n",
        "# M \\ddot P = - C( X - X_0)  - D \\dot X\n",
        "#  P = X  + Z(Y(t))\n",
        "#  Y(t)= vt\n",
        "\n",
        "#  v=60km/hour = 16.666 m / sec\n",
        "#\n",
        "#  M= 1000kg\n",
        "#  X_0 =  5cm\n",
        "#  C spring const so 1000kg @ 10 m/sec^2 gives 5 cm ie\n",
        "#    =   5 x 10e-06\n",
        "#  X roughly +- 10cm\n",
        "#  road 3 components period   1 sec, 2 sec, 4 sec (random amplitude)\n",
        "#  sin(Y/(16.666)),  cos(Y/(16.666)), sin(Y/(2*16.66)), cos(Y/(2*16.66))\n",
        "#  choose D0 to efold in 1 second ie. D/2M = 1.0 --> D=2000\n",
        "#  too small is 'bad'\n",
        "#  choose sampling rate @ 4 Hz\n",
        "#  magnitude road = +- 5 max * sin( Y / 16.66 m)\n",
        "\n",
        "#  10 minute samples = 600 x 4 points @ 4 Hz\n",
        "#\n",
        "\n",
        "def Zbase ( trigtype, period, K, Y):\n",
        "    \n",
        "    if trigtype=='sin':\n",
        "        return math.sin( K * period * Y)\n",
        "    if trigtype=='cos':\n",
        "        return math.cos( K * period * Y)\n",
        "    \n",
        "def Zbaseddot ( trigtype, period, K, Y, v):\n",
        "    \n",
        "#     d^2/dt^ (VT)=0\n",
        "    if trigtype=='sin':\n",
        "        return -math.cos( K * period * Y) * v * v * (K /period) * (K/period)\n",
        "    if trigtype=='cos':\n",
        "        return -math.sin( K * period * Y) * v * v * (K /period) * (K/period)\n",
        "    \n",
        "def Xdot(Xn, Xnm1, dT):\n",
        "    return (Xn - Xnm1)/dT\n",
        "\n",
        "def Xddot(Xn, Xnm1, Xnm2, dT):\n",
        "    return (Xn - 2 * Xnm1  + Xnm2)/(dT * dT)\n",
        "\n",
        "def getXnp1(LHS, M, D, C, Xn, Xnm1, dT):\n",
        "# solve for Xnp1\n",
        "#  LHS = M(Xnp1 - 2 Xn + Xnm1)/delT^2 + D(Xnp1 - Xnm1)/2delT + C Xn\n",
        "  \n",
        "    rval = (LHS * dT * dT - (M - D * dT/2) * Xnm1  + (2 *M - C * dT * dT) * Xn  )/(M + D * dT/2) \n",
        "    \n",
        "    lv = M* (rval - 2 * Xn + Xnm1)/(dT * dT) + D* (rval - Xnm1)/(2*dT) + C * Xn\n",
        "    \n",
        "#    print(\"check %f = %f\" % (LHS,lv))\n",
        "          \n",
        "    return  rval\n",
        "\n",
        "def getLHSval(Zddval, Ms, Vs, Cs, X0s):\n",
        "# Cs * X0 =spring force\n",
        "    return - Ms * Zddval + Cs * X0s\n",
        "\n",
        "def zRoad(coeffs, v, Y, period, maxfreq):\n",
        "    \n",
        "    zR =0\n",
        "   \n",
        "    if maxfreq >= 0.5:\n",
        "       zR = coeffs[0]* Zbaseddot('cos', period, 0.5, Y, v) + zR\n",
        "       zR = coeffs[1]* Zbaseddot('sin', period, 0.5, Y, v) + zR\n",
        "\n",
        "    if maxfreq >= 1.0:\n",
        "       zR = coeffs[2]* Zbaseddot('cos', period, 1.0, Y, v) + zR\n",
        "       zR = coeffs[3]* Zbaseddot('sin', period, 1.0, Y, v) + zR\n",
        "    \n",
        "    if maxfreq >=2.0:\n",
        "       zR = coeffs[4]* Zbaseddot('cos', period, 2.0, Y, v) + zR\n",
        "       zR = coeffs[5]* Zbaseddot('sin', period, 2.0, Y, v) + zR\n",
        "       \n",
        "    if maxfreq >=4.0:\n",
        "       zR = coeffs[6]* Zbaseddot('cos', period, 4.0, Y, v) + zR\n",
        "       zR = coeffs[7]* Zbaseddot('sin', period, 4.0, Y, v) + zR\n",
        "    \n",
        "    return zR\n",
        "\n",
        "def  getRandomCoeffs(N):\n",
        "    \n",
        "     ampChoice=[0.01, 0.02, 0.025, 0.03, 0.035]\n",
        "     coeffs=[]\n",
        "\n",
        "     for i in range(0,N):\n",
        "          rAmpl1=rn.choice(ampChoice) \n",
        "#          rAmpl1=rAmpl\n",
        "          coeffs.append(rAmpl1)\n",
        "          \n",
        "     return coeffs\n",
        " \n",
        " \n",
        "\n",
        "def  compute_sim(M, D0, V, C, X0, delT, period, maxfreq, coeffs, topsample):\n",
        "    \n",
        "    Y=0\n",
        "# start with spring at rest\n",
        "    Xnp1=X0\n",
        "    Xn=X0\n",
        "    Xnm1=X0\n",
        "\n",
        "\n",
        "    springPos=[]\n",
        "    timeVal=[]\n",
        "    roadSurf=[]\n",
        "    \n",
        "     \n",
        "    for i in range(0, topsample):\n",
        "    \n",
        "        t= i * delT\n",
        "        timeVal.append(t)\n",
        "#        print(\"                T=%8.2f\" % t)\n",
        "    \n",
        "        Y= V * t  \n",
        "    \n",
        "        Zddval= zRoad(coeffs, V, Y, period, maxfreq)\n",
        "    \n",
        "#        print(\"Zddval %f\" % Zddval)\n",
        "    \n",
        "# LHS= -M (ddot (Z(Y))) + CX0\n",
        "        LHS= getLHSval(Zddval, M, V, C, X0)\n",
        "        roadSurf.append(LHS)\n",
        "    \n",
        "        Xnp1 = getXnp1(LHS, M, D0, C, Xn, Xnm1, delT)\n",
        "        springPos.append(Xnp1)\n",
        "    \n",
        "#        print(\"Xnp1 %8.3f  Xn %8.3f Xnm1 %8.3f\" % (Xnp1, Xn, Xnm1))\n",
        "    \n",
        "        Xnm1=Xn\n",
        "        Xn=Xnp1\n",
        "        \n",
        "    return [roadSurf, timeVal, springPos, t]\n",
        "\n",
        "def add_sample(M, D0, V, C, X0, delT, period, maxfreq, botsample, topsample, lval,Ncoeffs, Rcoeffs, road_input_type):\n",
        " # compute discriminant\n",
        "    disc= D0*D0 - 4 * M * C   \n",
        "    \n",
        "    [roadSurf, timeVals, springPos, tmax]= compute_sim(M, D0, V, C, X0, delT, period, maxfreq, Rcoeffs, topsample)\n",
        "\n",
        "    yval=[]\n",
        "    Xdat=[]\n",
        "\n",
        "#    print(\"sample D0=%d label=%s\" % (D0,lval )) \n",
        "#    print(\"D=%12.2f disc  %12.2f   maxfreq= %f\" % (D0, disc,   maxfreq))\n",
        "#    if disc < 0:\n",
        "#        print(\"sqrt = %f\" % math.sqrt(-disc))\n",
        " \n",
        "# assume botsample is > 3\n",
        "\n",
        "    if road_input_type=='vibration':\n",
        "        tupleLen=8\n",
        "    elif road_input_type=='surface':\n",
        "        tupleLen=2\n",
        "    else:\n",
        "        print(\"unknown road_type %s\" % road_type)\n",
        "        tupleLen=2\n",
        " \n",
        "# in this case include roadSurf = LHS as variable\n",
        "    xnorm=10000 \n",
        "    for i in range(botsample,topsample):\n",
        "        if tupleLen==2:\n",
        "# road input\n",
        "           Xdat.append([roadSurf[i]/xnorm, springPos[i-2], springPos[i-1], springPos[i]])\n",
        "# in vehicle vibration\n",
        "        elif tupleLen==5:\n",
        "           Xdat.append([springPos[i-5], springPos[i-4], springPos[i-3], springPos[i-2], springPos[i-1], springPos[i]])\n",
        "#  in vehicle vibration (long)\n",
        "        elif tupleLen==8:\n",
        "           Xdat.append([springPos[-8], springPos[-7], springPos[-6], springPos[i-5], springPos[i-4], springPos[i-3], springPos[i-2], springPos[i-1], springPos[i]])\n",
        "        else:\n",
        "            print(\"Unsupported tupleLen %d\" % tupleLen)\n",
        "            return [[],[]]\n",
        "        \n",
        "        yval.append(lval)\n",
        " \n",
        "    return [Xdat, yval]\n",
        "\n",
        "# This main function generates data based on user's selection of parameters    \n",
        "def generate_data(number_runs, experiment_type, observation_type):\n",
        "\n",
        "  # mass    \n",
        "  M=2000 \n",
        "  #   5cm compression\n",
        "  X0= 0.05\n",
        "  # spring const    \n",
        "\n",
        "  C= 0.6  * 10e+04\n",
        "  delT=0.25\n",
        "  # interesting values 500, 5000, 15000, 25000\n",
        "  \n",
        "  # damping\n",
        "  Dvalues=[5000, 5500, 6000, 6500, 4500, 4000, 3500, 500, 600, 700, 800, 400, 300, 200]\n",
        "\n",
        "  LABELvalues=['good','good', 'good', 'good', 'good','good','good','bad','bad','bad','bad','bad','bad','bad']\n",
        "\n",
        "  dindexlist=[x for x in range(0,14)]\n",
        "\n",
        "  \n",
        "  # car moves at 16.66 m/s\n",
        "  V= 16.66 \n",
        "  period= 16.66 \n",
        "  maxfreq=4.0\n",
        "  \n",
        "  botsample=400 \n",
        "  topsample=500\n",
        "  Ncoeffs=8\n",
        "\n",
        "  X=[]\n",
        "  y=[]\n",
        "  ngood=0\n",
        "  nbad=0\n",
        "\n",
        "  nruns = number_runs or 500\n",
        "  experiment_type= experiment_type or 'random_roads'\n",
        "  # experiment_type='standard_road'\n",
        "\n",
        "  # initialize road\n",
        "  Rcoeffs=getRandomCoeffs(Ncoeffs) \n",
        "\n",
        "  ldindexlist=[x for x in range(0, 100* nruns)]\n",
        "\n",
        "  for i in range(0,nruns):\n",
        "      \n",
        "            dindex=rn.choice(dindexlist)\n",
        "\n",
        "            D0=Dvalues[dindex]\n",
        "            labval=LABELvalues[dindex]\n",
        "      \n",
        "  \n",
        "            if experiment_type=='random_roads':\n",
        "                Rcoeffs=getRandomCoeffs(Ncoeffs)\n",
        "                \n",
        "\n",
        "  \n",
        "  #  For  in vehicle vibration road_input_type='vibration'\n",
        "  #  For  road input set road_input_type='surface'\n",
        "            \n",
        "            road_input_type = observation_type or 'surface'\n",
        "            #  road_input_type='vibration'\n",
        "  \n",
        "            [Xdat, yval]= add_sample(M, D0, V, C, X0, delT, period, maxfreq, botsample, topsample, labval,Ncoeffs, Rcoeffs, road_input_type)\n",
        "            \n",
        "  # [roadsurf, Xn-2, Xn-1, Xn]\n",
        "            if yval[0]=='good':\n",
        "                ngood+=1\n",
        "            if yval[0]=='bad':\n",
        "                nbad+=1\n",
        "  \n",
        "            for j in range(0, len(Xdat)):\n",
        "                  X.append(Xdat[j])\n",
        "                  y.append(yval[j])\n",
        "                  \n",
        "\n",
        "  print(\"shuffling %d entries\" % len(ldindexlist))\n",
        "  rn.shuffle(ldindexlist)\n",
        "  # sanity check\n",
        "  print(ldindexlist[0:10])\n",
        "\n",
        "  X_rn=[]\n",
        "  y_rn=[]\n",
        "\n",
        "  for i in range(0, len(X)):\n",
        "      X_rn.append(X[ldindexlist[i]])\n",
        "      y_rn.append(y[ldindexlist[i]])\n",
        "      \n",
        "      \n",
        "            \n",
        "  print(\"made %d samples with botsample %d topsample %d\" % (nruns,botsample, topsample))\n",
        "  print(\"maxfreq= %8.2f M= %8.2f V=%8.2f C=%8.2f\" % (maxfreq, M, V, C))\n",
        "\n",
        "  totalN=len(X)\n",
        "  print(\"Total samples %d good runs %d bad runs %d\" % (totalN, ngood, nbad))\n",
        "\n",
        "  #print(Xdat)\n",
        "\n",
        "  print (strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
        "\n",
        "  return X_rn, y_rn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lhBJiYa1V3h"
      },
      "source": [
        "# Analysis\n",
        "\n",
        "*Definitions of Accuracy and Error Rate*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAPMBWUBqhyd"
      },
      "source": [
        "The confusion matrix is a common metric to measure the performance of a classification algorithm. In this case, only \"good\" and \"bad\" are labelled (binary). So the confusion matrix can be shown as below:\n",
        "\n",
        "|               | Predicted(No) | Predicted(Yes)  |\n",
        "| ------------- |:---------------:| -----------------:|\n",
        "| **Actual(No)**   | True Negatives (TN) | False Positives (FP) |\n",
        "| **Actual(Yes)**  | False Negatives (FN) | True Positives (TP) |\n",
        "\n",
        "From the matrix, we could get the Accuracy by this formula:\n",
        "$$ Accuracy = \\frac {TP + TN}{TP + TN + FP + FN} $$\n",
        "\n",
        "And the Error rate would be $1-Accuracy$, same as\n",
        "$$ ErrorRate = \\frac {FP + FN}{TP + TN + FP + FN} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfJZFK8P6bsd"
      },
      "source": [
        "## Model Selection\n",
        "----\n",
        "\n",
        "Here is the summary table of all model candidates ranked by the Accuracy (via PyCaret)\n",
        "\n",
        "*Based on `500 N runs, random roads, and road input` *\n",
        "\n",
        "|Model|Accuracy|AUC|Recall|Prec.|F1|Kappa|MCC|TT (Sec)|\n",
        "|--- |--- |--- |--- |--- |--- |--- |--- |--- |\n",
        "|Extra Trees Classifier|0.9791|0.9983|0.9789|0.9806|0.9797|0.9582|0.9582|1.709|\n",
        "|Quadratic Discriminant Analysis|0.9783|0.9998|0.9579|1.0000|0.9785|0.9566|0.9576|0.036|\n",
        "|K Neighbors Classifier|0.9729|0.9955|0.9731|0.9742|0.9736|0.9457|0.9457|0.213|\n",
        "|Random Forest Classifier|0.9727|0.9969|0.9710|0.9759|0.9735|0.9454|0.9455|7.500|\n",
        "|Decision Tree Classifier|0.9477|0.9477|0.9472|0.9510|0.9491|0.8953|0.8953|0.244|\n",
        "|Light Gradient Boosting Machine|0.9339|0.9893|0.9651|0.9118|0.9376|0.8674|0.8691|0.360|\n",
        "|Gradient Boosting Classifier|0.8409|0.9444|0.9266|0.7973|0.8570|0.6799|0.6898|4.073|\n",
        "|Ada Boost Classifier|0.6967|0.7706|0.8287|0.6648|0.7377|0.3883|0.4018|1.082|\n",
        "|Naive Bayes|0.6476|0.6815|0.7716|0.6284|0.6926|0.2898|0.2983|0.032|\n",
        "|Linear Discriminant Analysis|0.5494|0.5109|0.9391|0.5356|0.6820|0.0768|0.1267|0.049|\n",
        "|Ridge Classifier|0.5278|0.0000|0.9967|0.5217|0.6849|0.0279|0.0915|0.031|\n",
        "|Logistic Regression|0.5148|0.5109|1.0000|0.5148|0.6797|0.0003|0.0039|0.379|\n",
        "|SVM - Linear Kernel|0.5147|0.0000|1.0000|0.5147|0.6796|0.0000|0.0000|0.062|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1XQJHld6iQW"
      },
      "source": [
        "Tree models and KNN have very good accuracy compared to other models. However, most of tree classifiers need more time to compute (takes seconds). Therefore, I chose the common **KNN model** as the estimator in this case, which has a pretty good Accuracy and AUC as well as short training time (0.213s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khXusr_hwnU0"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSylfLynFt8g"
      },
      "source": [
        "## Define Function for Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2U5I0OopZ-q"
      },
      "source": [
        "# Create a function to generate model result\n",
        "def ml_result_report(y_test, y_pred):\n",
        "  # Create Confusion Matrix with testing and prediction data\n",
        "  Mc = metrics.confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  totalN = Mc[0][0] + Mc[0][1] + Mc[1][0] + Mc[1][1]\n",
        "  misclassifiedN = Mc[0][1] + Mc[1][0]\n",
        "\n",
        "  errorRate = misclassifiedN / totalN\n",
        "\n",
        "  print(\"confusion matrix: on test data set\")\n",
        "  print(Mc)\n",
        "\n",
        "  print(\"errorRate %5.3f\" % errorRate)\n",
        "  # print (strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
        "  print(\"N=%d : experiment_type: %s  road_input_type: %s\" % (number_runs, experiment_type, observation_type))\n",
        "\n",
        "  return errorRate"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY3edCGrF45i"
      },
      "source": [
        "## Define a Model Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EFUs3zj152o"
      },
      "source": [
        "# Create a pipeline to train model and then predict\n",
        "def model_pipeline(X_rn, y_rn, model):\n",
        "  # Split samples\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_rn, y_rn, test_size=testfraction, random_state=42)\n",
        "\n",
        "  # Standardization of sample\n",
        "  X_scaler = StandardScaler().fit(X_train)\n",
        "  X_train_scaled = X_scaler.transform(X_train)\n",
        "  X_test_scaled = X_scaler.transform(X_test)\n",
        "\n",
        "  # Train model\n",
        "  model.fit(X_train_scaled, y_train)\n",
        "\n",
        "  # Print out score\n",
        "  train_score = model.score(X_train_scaled, y_train)\n",
        "  model_score = model.score(X_test_scaled, y_test)\n",
        "  print(\"Train score: %5.4f\" % train_score)\n",
        "  print(\"Model score: %5.4f\" % model_score)\n",
        "\n",
        "  # Get predict data\n",
        "  y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "  return y_test, y_pred"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGo0Rec1F8TI"
      },
      "source": [
        "## Define Main Function to Run 4 Replications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPZWTID3E88n"
      },
      "source": [
        "# A main function to generate avg error rate\n",
        "def compute_avg_err_rate(model):\n",
        "\n",
        "  list_err_rate = []\n",
        "\n",
        "  # Main function to generate required data\n",
        "  for i in range(4):\n",
        "    X_rn, y_rn = generate_data(number_runs, experiment_type, observation_type)\n",
        "\n",
        "    y_test, y_pred = model_pipeline(X_rn, y_rn, model)\n",
        "    err_rate = ml_result_report(y_test, y_pred)\n",
        "\n",
        "    list_err_rate.append(err_rate)\n",
        "    print(\"========== %s out of 4 replications========\" % (i+1))\n",
        "\n",
        "  print(\"The average error rate of 4 replications is:\")\n",
        "  print(sum(list_err_rate)/len(list_err_rate))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glroNK1BFhJ1"
      },
      "source": [
        "# Generate Data and Return result with 3 Models\n",
        "\n",
        "3 different models were built and ran for 4 times for each to calculate the error rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX2g3LGZJ2X6"
      },
      "source": [
        "## Parameter Settings for Each Case\n",
        "\n",
        "N runs, Experiment type, Observation type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPNp5dJw9V16"
      },
      "source": [
        "#====================================================\n",
        "# Here is the list of parameter variables\n",
        "# Change the following variable values for each case\n",
        "#----------------------------------------------------\n",
        "number_runs = 500\n",
        "# experiment_type = \"standard_road\"\n",
        "experiment_type = \"random_roads\"\n",
        "\n",
        "# observation_type = \"surface\"\n",
        "observation_type = \"vibration\"\n",
        "testfraction = 0.3\n",
        "#===================================================="
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtTd5_ykKDoX"
      },
      "source": [
        "## KNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7k5MyPu74RKX",
        "outputId": "95d6c22f-9c13-413f-cb53-a1e9f67fa867"
      },
      "source": [
        "# KNN\n",
        "# A n_neighbors vs accuracy plotting was done separately\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "compute_avg_err_rate(knn)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shuffling 50000 entries\n",
            "[36867, 19471, 28194, 10806, 7384, 45232, 8538, 18767, 45858, 33489]\n",
            "made 500 samples with botsample 400 topsample 500\n",
            "maxfreq=     4.00 M=  2000.00 V=   16.66 C=60000.00\n",
            "Total samples 50000 good runs 276 bad runs 224\n",
            "2021-05-22 03:34:16\n",
            "Train score: 0.9919\n",
            "Model score: 0.9810\n",
            "confusion matrix: on test data set\n",
            "[[6557  172]\n",
            " [ 113 8158]]\n",
            "errorRate 0.019\n",
            "N=500 : experiment_type: random_roads  road_input_type: surface\n",
            "========== 1 out of 4 replications========\n",
            "shuffling 50000 entries\n",
            "[26961, 14535, 40783, 11357, 960, 23015, 23381, 28512, 4891, 11422]\n",
            "made 500 samples with botsample 400 topsample 500\n",
            "maxfreq=     4.00 M=  2000.00 V=   16.66 C=60000.00\n",
            "Total samples 50000 good runs 245 bad runs 255\n",
            "2021-05-22 03:34:20\n",
            "Train score: 0.9917\n",
            "Model score: 0.9819\n",
            "confusion matrix: on test data set\n",
            "[[7595  135]\n",
            " [ 136 7134]]\n",
            "errorRate 0.018\n",
            "N=500 : experiment_type: random_roads  road_input_type: surface\n",
            "========== 2 out of 4 replications========\n",
            "shuffling 50000 entries\n",
            "[47470, 9887, 39341, 23228, 20070, 33651, 45633, 12605, 20394, 9319]\n",
            "made 500 samples with botsample 400 topsample 500\n",
            "maxfreq=     4.00 M=  2000.00 V=   16.66 C=60000.00\n",
            "Total samples 50000 good runs 235 bad runs 265\n",
            "2021-05-22 03:34:24\n",
            "Train score: 0.9915\n",
            "Model score: 0.9809\n",
            "confusion matrix: on test data set\n",
            "[[7830  126]\n",
            " [ 160 6884]]\n",
            "errorRate 0.019\n",
            "N=500 : experiment_type: random_roads  road_input_type: surface\n",
            "========== 3 out of 4 replications========\n",
            "shuffling 50000 entries\n",
            "[44107, 10659, 14686, 13563, 20574, 15656, 34654, 47319, 31487, 11567]\n",
            "made 500 samples with botsample 400 topsample 500\n",
            "maxfreq=     4.00 M=  2000.00 V=   16.66 C=60000.00\n",
            "Total samples 50000 good runs 250 bad runs 250\n",
            "2021-05-22 03:34:28\n",
            "Train score: 0.9915\n",
            "Model score: 0.9797\n",
            "confusion matrix: on test data set\n",
            "[[7411  159]\n",
            " [ 145 7285]]\n",
            "errorRate 0.020\n",
            "N=500 : experiment_type: random_roads  road_input_type: surface\n",
            "========== 4 out of 4 replications========\n",
            "The average error rate of 4 replications is:\n",
            "0.0191\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLf83g6ZKHL-"
      },
      "source": [
        "## Naive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6Q1J0RQ4RKb",
        "outputId": "4d35d4bc-44bc-46d6-fc19-c6725e795299"
      },
      "source": [
        "# NB\n",
        "nb = GaussianNB()\n",
        "compute_avg_err_rate(nb)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shuffling 100000 entries\n",
            "[66003, 89400, 44150, 80300, 1500, 46966, 73362, 41893, 74727, 64702]\n",
            "made 1000 samples with botsample 400 topsample 500\n",
            "maxfreq=     4.00 M=  2000.00 V=   16.66 C=60000.00\n",
            "Total samples 100000 good runs 483 bad runs 517\n",
            "2021-05-23 01:50:48\n",
            "Train score: 0.6414\n",
            "Model score: 0.6378\n",
            "confusion matrix: on test data set\n",
            "[[10776  4834]\n",
            " [ 6033  8357]]\n",
            "errorRate 0.362\n",
            "N=1000 : experiment_type: random_roads  road_input_type: surface\n",
            "========== 1 out of 4 replications========\n",
            "shuffling 100000 entries\n",
            "[36759, 8844, 19886, 38862, 32614, 82245, 51873, 18445, 9711, 86001]\n",
            "made 1000 samples with botsample 400 topsample 500\n",
            "maxfreq=     4.00 M=  2000.00 V=   16.66 C=60000.00\n",
            "Total samples 100000 good runs 506 bad runs 494\n",
            "2021-05-23 01:50:52\n",
            "Train score: 0.6531\n",
            "Model score: 0.6518\n",
            "confusion matrix: on test data set\n",
            "[[ 8801  5995]\n",
            " [ 4452 10752]]\n",
            "errorRate 0.348\n",
            "N=1000 : experiment_type: random_roads  road_input_type: surface\n",
            "========== 2 out of 4 replications========\n",
            "shuffling 100000 entries\n",
            "[14499, 22025, 41805, 42371, 69749, 90009, 49313, 72441, 87028, 31484]\n",
            "made 1000 samples with botsample 400 topsample 500\n",
            "maxfreq=     4.00 M=  2000.00 V=   16.66 C=60000.00\n",
            "Total samples 100000 good runs 507 bad runs 493\n",
            "2021-05-23 01:50:57\n",
            "Train score: 0.6508\n",
            "Model score: 0.6539\n",
            "confusion matrix: on test data set\n",
            "[[ 8459  6332]\n",
            " [ 4052 11157]]\n",
            "errorRate 0.346\n",
            "N=1000 : experiment_type: random_roads  road_input_type: surface\n",
            "========== 3 out of 4 replications========\n",
            "shuffling 100000 entries\n",
            "[813, 49104, 71247, 82700, 87204, 92557, 65978, 92371, 19570, 21652]\n",
            "made 1000 samples with botsample 400 topsample 500\n",
            "maxfreq=     4.00 M=  2000.00 V=   16.66 C=60000.00\n",
            "Total samples 100000 good runs 489 bad runs 511\n",
            "2021-05-23 01:51:02\n",
            "Train score: 0.6405\n",
            "Model score: 0.6387\n",
            "confusion matrix: on test data set\n",
            "[[9997 5398]\n",
            " [5440 9165]]\n",
            "errorRate 0.361\n",
            "N=1000 : experiment_type: random_roads  road_input_type: surface\n",
            "========== 4 out of 4 replications========\n",
            "The average error rate of 4 replications is:\n",
            "0.35446666666666665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blXHIrju4ctv"
      },
      "source": [
        "## Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJVz2eBXm0mJ"
      },
      "source": [
        "def nn_compute_avg_err_rate():\n",
        "\n",
        "  list_err_rate = []\n",
        "  # Main function to generate required data\n",
        "  for i in range(4):\n",
        "    X_rn, y_rn = generate_data(number_runs, experiment_type, observation_type)\n",
        "    \n",
        "    # Split samples\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_rn, y_rn, random_state=42)\n",
        "\n",
        "    # Standardization\n",
        "    X_scaler = StandardScaler().fit(X_train)\n",
        "    X_train_scaled = X_scaler.transform(X_train)\n",
        "    X_test_scaled = X_scaler.transform(X_test)\n",
        "\n",
        "    # Transform y values\n",
        "    label_model = LabelEncoder()\n",
        "    label_model.fit(y_train)\n",
        "\n",
        "    y_train_encoded = label_model.transform(y_train)\n",
        "    y_test_encoded = label_model.transform(y_test)\n",
        "\n",
        "    # Convert to categorical data\n",
        "    y_train_categorical = to_categorical(y_train_encoded)\n",
        "    y_test_categorical = to_categorical(y_test_encoded)\n",
        "\n",
        "    # Building NN model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=9, activation=\"relu\", input_dim=9))\n",
        "    model.add(Dense(units=20, activation=\"relu\"))\n",
        "    model.add(Dense(units=5, activation=\"relu\"))\n",
        "    model.add(Dense(units=2, activation=\"softmax\"))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "    # Fitting model\n",
        "    model.fit(X_train_scaled, y_train_categorical, epochs=50, shuffle=True, verbose=2)\n",
        "\n",
        "    # Predit values\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_pred\n",
        "\n",
        "    # confusion_matrix(y_test_categorical.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "    err_rate = ml_result_report(y_test_categorical.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "\n",
        "    list_err_rate.append(err_rate)\n",
        "    print(\"========== %s out of 4 replications========\" % (i+1))\n",
        "\n",
        "  print(\"The average error rate of 4 replications is:\")\n",
        "  print(sum(list_err_rate)/len(list_err_rate))\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc_b3HwEM_8z",
        "outputId": "443dd2ad-80b0-4686-86be-9bed156e7234"
      },
      "source": [
        "# Run MLP function\n",
        "nn_compute_avg_err_rate()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shuffling 50000 entries\n",
            "[11444, 48587, 45888, 33057, 30826, 20910, 42076, 4317, 12940, 9973]\n",
            "made 500 samples with botsample 400 topsample 500\n",
            "maxfreq=     4.00 M=  2000.00 V=   16.66 C=60000.00\n",
            "Total samples 50000 good runs 257 bad runs 243\n",
            "2021-05-23 03:13:03\n",
            "Epoch 1/50\n",
            "1172/1172 - 2s - loss: 0.3763 - accuracy: 0.8236\n",
            "Epoch 2/50\n",
            "1172/1172 - 1s - loss: 0.2906 - accuracy: 0.8668\n",
            "Epoch 3/50\n",
            "1172/1172 - 1s - loss: 0.2377 - accuracy: 0.8863\n",
            "Epoch 4/50\n",
            "1172/1172 - 1s - loss: 0.2202 - accuracy: 0.8916\n",
            "Epoch 5/50\n",
            "1172/1172 - 1s - loss: 0.2106 - accuracy: 0.8959\n",
            "Epoch 6/50\n",
            "1172/1172 - 1s - loss: 0.2035 - accuracy: 0.8992\n",
            "Epoch 7/50\n",
            "1172/1172 - 1s - loss: 0.1995 - accuracy: 0.8998\n",
            "Epoch 8/50\n",
            "1172/1172 - 1s - loss: 0.1947 - accuracy: 0.9026\n",
            "Epoch 9/50\n",
            "1172/1172 - 1s - loss: 0.1927 - accuracy: 0.9022\n",
            "Epoch 10/50\n",
            "1172/1172 - 1s - loss: 0.1900 - accuracy: 0.9039\n",
            "Epoch 11/50\n",
            "1172/1172 - 1s - loss: 0.1876 - accuracy: 0.9062\n",
            "Epoch 12/50\n",
            "1172/1172 - 1s - loss: 0.1862 - accuracy: 0.9067\n",
            "Epoch 13/50\n",
            "1172/1172 - 1s - loss: 0.1846 - accuracy: 0.9085\n",
            "Epoch 14/50\n",
            "1172/1172 - 1s - loss: 0.1833 - accuracy: 0.9084\n",
            "Epoch 15/50\n",
            "1172/1172 - 1s - loss: 0.1811 - accuracy: 0.9096\n",
            "Epoch 16/50\n",
            "1172/1172 - 1s - loss: 0.1799 - accuracy: 0.9087\n",
            "Epoch 17/50\n",
            "1172/1172 - 1s - loss: 0.1780 - accuracy: 0.9102\n",
            "Epoch 18/50\n",
            "1172/1172 - 1s - loss: 0.1764 - accuracy: 0.9106\n",
            "Epoch 19/50\n",
            "1172/1172 - 1s - loss: 0.1746 - accuracy: 0.9095\n",
            "Epoch 20/50\n",
            "1172/1172 - 1s - loss: 0.1718 - accuracy: 0.9107\n",
            "Epoch 21/50\n",
            "1172/1172 - 1s - loss: 0.1678 - accuracy: 0.9129\n",
            "Epoch 22/50\n",
            "1172/1172 - 1s - loss: 0.1655 - accuracy: 0.9126\n",
            "Epoch 23/50\n",
            "1172/1172 - 1s - loss: 0.1616 - accuracy: 0.9146\n",
            "Epoch 24/50\n",
            "1172/1172 - 1s - loss: 0.1582 - accuracy: 0.9185\n",
            "Epoch 25/50\n",
            "1172/1172 - 1s - loss: 0.1559 - accuracy: 0.9176\n",
            "Epoch 26/50\n",
            "1172/1172 - 1s - loss: 0.1529 - accuracy: 0.9204\n",
            "Epoch 27/50\n",
            "1172/1172 - 1s - loss: 0.1489 - accuracy: 0.9227\n",
            "Epoch 28/50\n",
            "1172/1172 - 1s - loss: 0.1466 - accuracy: 0.9258\n",
            "Epoch 29/50\n",
            "1172/1172 - 1s - loss: 0.1429 - accuracy: 0.9287\n",
            "Epoch 30/50\n",
            "1172/1172 - 1s - loss: 0.1402 - accuracy: 0.9308\n",
            "Epoch 31/50\n",
            "1172/1172 - 1s - loss: 0.1380 - accuracy: 0.9327\n",
            "Epoch 32/50\n",
            "1172/1172 - 1s - loss: 0.1362 - accuracy: 0.9333\n",
            "Epoch 33/50\n",
            "1172/1172 - 1s - loss: 0.1346 - accuracy: 0.9350\n",
            "Epoch 34/50\n",
            "1172/1172 - 1s - loss: 0.1325 - accuracy: 0.9351\n",
            "Epoch 35/50\n",
            "1172/1172 - 1s - loss: 0.1316 - accuracy: 0.9354\n",
            "Epoch 36/50\n",
            "1172/1172 - 1s - loss: 0.1304 - accuracy: 0.9378\n",
            "Epoch 37/50\n",
            "1172/1172 - 1s - loss: 0.1295 - accuracy: 0.9366\n",
            "Epoch 38/50\n",
            "1172/1172 - 1s - loss: 0.1280 - accuracy: 0.9385\n",
            "Epoch 39/50\n",
            "1172/1172 - 1s - loss: 0.1265 - accuracy: 0.9395\n",
            "Epoch 40/50\n",
            "1172/1172 - 1s - loss: 0.1257 - accuracy: 0.9392\n",
            "Epoch 41/50\n",
            "1172/1172 - 1s - loss: 0.1254 - accuracy: 0.9397\n",
            "Epoch 42/50\n",
            "1172/1172 - 1s - loss: 0.1245 - accuracy: 0.9402\n",
            "Epoch 43/50\n",
            "1172/1172 - 1s - loss: 0.1234 - accuracy: 0.9401\n",
            "Epoch 44/50\n",
            "1172/1172 - 1s - loss: 0.1230 - accuracy: 0.9412\n",
            "Epoch 45/50\n",
            "1172/1172 - 1s - loss: 0.1223 - accuracy: 0.9406\n",
            "Epoch 46/50\n",
            "1172/1172 - 1s - loss: 0.1213 - accuracy: 0.9411\n",
            "Epoch 47/50\n",
            "1172/1172 - 1s - loss: 0.1206 - accuracy: 0.9421\n",
            "Epoch 48/50\n",
            "1172/1172 - 1s - loss: 0.1199 - accuracy: 0.9422\n",
            "Epoch 49/50\n",
            "1172/1172 - 1s - loss: 0.1198 - accuracy: 0.9420\n",
            "Epoch 50/50\n",
            "1172/1172 - 1s - loss: 0.1188 - accuracy: 0.9436\n",
            "confusion matrix: on test data set\n",
            "[[5663  365]\n",
            " [ 374 6098]]\n",
            "errorRate 0.059\n",
            "N=500 : experiment_type: random_roads  road_input_type: vibration\n",
            "========== 1 out of 4 replications========\n",
            "shuffling 50000 entries\n",
            "[10275, 30742, 24647, 5349, 20848, 493, 47338, 43159, 22909, 29787]\n",
            "made 500 samples with botsample 400 topsample 500\n",
            "maxfreq=     4.00 M=  2000.00 V=   16.66 C=60000.00\n",
            "Total samples 50000 good runs 261 bad runs 239\n",
            "2021-05-23 03:14:03\n",
            "Epoch 1/50\n",
            "1172/1172 - 2s - loss: 0.3475 - accuracy: 0.8655\n",
            "Epoch 2/50\n",
            "1172/1172 - 1s - loss: 0.2434 - accuracy: 0.8958\n",
            "Epoch 3/50\n",
            "1172/1172 - 1s - loss: 0.2137 - accuracy: 0.8998\n",
            "Epoch 4/50\n",
            "1172/1172 - 1s - loss: 0.1959 - accuracy: 0.9016\n",
            "Epoch 5/50\n",
            "1172/1172 - 1s - loss: 0.1834 - accuracy: 0.9030\n",
            "Epoch 6/50\n",
            "1172/1172 - 1s - loss: 0.1763 - accuracy: 0.9126\n",
            "Epoch 7/50\n",
            "1172/1172 - 1s - loss: 0.1711 - accuracy: 0.9167\n",
            "Epoch 8/50\n",
            "1172/1172 - 1s - loss: 0.1663 - accuracy: 0.9193\n",
            "Epoch 9/50\n",
            "1172/1172 - 1s - loss: 0.1620 - accuracy: 0.9214\n",
            "Epoch 10/50\n",
            "1172/1172 - 1s - loss: 0.1568 - accuracy: 0.9234\n",
            "Epoch 11/50\n",
            "1172/1172 - 1s - loss: 0.1542 - accuracy: 0.9258\n",
            "Epoch 12/50\n",
            "1172/1172 - 1s - loss: 0.1519 - accuracy: 0.9273\n",
            "Epoch 13/50\n",
            "1172/1172 - 1s - loss: 0.1490 - accuracy: 0.9294\n",
            "Epoch 14/50\n",
            "1172/1172 - 1s - loss: 0.1467 - accuracy: 0.9303\n",
            "Epoch 15/50\n",
            "1172/1172 - 1s - loss: 0.1439 - accuracy: 0.9315\n",
            "Epoch 16/50\n",
            "1172/1172 - 1s - loss: 0.1403 - accuracy: 0.9355\n",
            "Epoch 17/50\n",
            "1172/1172 - 1s - loss: 0.1371 - accuracy: 0.9363\n",
            "Epoch 18/50\n",
            "1172/1172 - 1s - loss: 0.1344 - accuracy: 0.9350\n",
            "Epoch 19/50\n",
            "1172/1172 - 1s - loss: 0.1328 - accuracy: 0.9377\n",
            "Epoch 20/50\n",
            "1172/1172 - 1s - loss: 0.1310 - accuracy: 0.9371\n",
            "Epoch 21/50\n",
            "1172/1172 - 1s - loss: 0.1287 - accuracy: 0.9382\n",
            "Epoch 22/50\n",
            "1172/1172 - 1s - loss: 0.1265 - accuracy: 0.9402\n",
            "Epoch 23/50\n",
            "1172/1172 - 1s - loss: 0.1251 - accuracy: 0.9395\n",
            "Epoch 24/50\n",
            "1172/1172 - 1s - loss: 0.1236 - accuracy: 0.9406\n",
            "Epoch 25/50\n",
            "1172/1172 - 1s - loss: 0.1223 - accuracy: 0.9404\n",
            "Epoch 26/50\n",
            "1172/1172 - 1s - loss: 0.1209 - accuracy: 0.9418\n",
            "Epoch 27/50\n",
            "1172/1172 - 1s - loss: 0.1205 - accuracy: 0.9414\n",
            "Epoch 28/50\n",
            "1172/1172 - 1s - loss: 0.1181 - accuracy: 0.9427\n",
            "Epoch 29/50\n",
            "1172/1172 - 1s - loss: 0.1179 - accuracy: 0.9420\n",
            "Epoch 30/50\n",
            "1172/1172 - 1s - loss: 0.1169 - accuracy: 0.9430\n",
            "Epoch 31/50\n",
            "1172/1172 - 1s - loss: 0.1163 - accuracy: 0.9439\n",
            "Epoch 32/50\n",
            "1172/1172 - 1s - loss: 0.1155 - accuracy: 0.9440\n",
            "Epoch 33/50\n",
            "1172/1172 - 1s - loss: 0.1150 - accuracy: 0.9449\n",
            "Epoch 34/50\n",
            "1172/1172 - 1s - loss: 0.1138 - accuracy: 0.9442\n",
            "Epoch 35/50\n",
            "1172/1172 - 1s - loss: 0.1126 - accuracy: 0.9451\n",
            "Epoch 36/50\n",
            "1172/1172 - 1s - loss: 0.1124 - accuracy: 0.9439\n",
            "Epoch 37/50\n",
            "1172/1172 - 1s - loss: 0.1125 - accuracy: 0.9450\n",
            "Epoch 38/50\n",
            "1172/1172 - 1s - loss: 0.1100 - accuracy: 0.9463\n",
            "Epoch 39/50\n",
            "1172/1172 - 1s - loss: 0.1098 - accuracy: 0.9449\n",
            "Epoch 40/50\n",
            "1172/1172 - 1s - loss: 0.1100 - accuracy: 0.9466\n",
            "Epoch 41/50\n",
            "1172/1172 - 1s - loss: 0.1089 - accuracy: 0.9472\n",
            "Epoch 42/50\n",
            "1172/1172 - 1s - loss: 0.1082 - accuracy: 0.9472\n",
            "Epoch 43/50\n",
            "1172/1172 - 1s - loss: 0.1068 - accuracy: 0.9477\n",
            "Epoch 44/50\n",
            "1172/1172 - 1s - loss: 0.1072 - accuracy: 0.9478\n",
            "Epoch 45/50\n",
            "1172/1172 - 1s - loss: 0.1059 - accuracy: 0.9482\n",
            "Epoch 46/50\n",
            "1172/1172 - 1s - loss: 0.1054 - accuracy: 0.9492\n",
            "Epoch 47/50\n",
            "1172/1172 - 1s - loss: 0.1038 - accuracy: 0.9491\n",
            "Epoch 48/50\n",
            "1172/1172 - 1s - loss: 0.1028 - accuracy: 0.9501\n",
            "Epoch 49/50\n",
            "1172/1172 - 1s - loss: 0.1019 - accuracy: 0.9481\n",
            "Epoch 50/50\n",
            "1172/1172 - 1s - loss: 0.1009 - accuracy: 0.9506\n",
            "confusion matrix: on test data set\n",
            "[[5691  244]\n",
            " [ 307 6258]]\n",
            "errorRate 0.044\n",
            "N=500 : experiment_type: random_roads  road_input_type: vibration\n",
            "========== 2 out of 4 replications========\n",
            "shuffling 50000 entries\n",
            "[19379, 35897, 13541, 24683, 36989, 17827, 28632, 23225, 28003, 1449]\n",
            "made 500 samples with botsample 400 topsample 500\n",
            "maxfreq=     4.00 M=  2000.00 V=   16.66 C=60000.00\n",
            "Total samples 50000 good runs 240 bad runs 260\n",
            "2021-05-23 03:15:04\n",
            "Epoch 1/50\n",
            "1172/1172 - 2s - loss: 0.3416 - accuracy: 0.8298\n",
            "Epoch 2/50\n",
            "1172/1172 - 1s - loss: 0.2703 - accuracy: 0.8687\n",
            "Epoch 3/50\n",
            "1172/1172 - 1s - loss: 0.2366 - accuracy: 0.8841\n",
            "Epoch 4/50\n",
            "1172/1172 - 1s - loss: 0.2132 - accuracy: 0.8995\n",
            "Epoch 5/50\n",
            "1172/1172 - 1s - loss: 0.1949 - accuracy: 0.9096\n",
            "Epoch 6/50\n",
            "1172/1172 - 1s - loss: 0.1833 - accuracy: 0.9142\n",
            "Epoch 7/50\n",
            "1172/1172 - 1s - loss: 0.1767 - accuracy: 0.9153\n",
            "Epoch 8/50\n",
            "1172/1172 - 1s - loss: 0.1712 - accuracy: 0.9164\n",
            "Epoch 9/50\n",
            "1172/1172 - 1s - loss: 0.1647 - accuracy: 0.9189\n",
            "Epoch 10/50\n",
            "1172/1172 - 1s - loss: 0.1601 - accuracy: 0.9212\n",
            "Epoch 11/50\n",
            "1172/1172 - 1s - loss: 0.1576 - accuracy: 0.9226\n",
            "Epoch 12/50\n",
            "1172/1172 - 1s - loss: 0.1532 - accuracy: 0.9257\n",
            "Epoch 13/50\n",
            "1172/1172 - 1s - loss: 0.1511 - accuracy: 0.9250\n",
            "Epoch 14/50\n",
            "1172/1172 - 1s - loss: 0.1478 - accuracy: 0.9272\n",
            "Epoch 15/50\n",
            "1172/1172 - 1s - loss: 0.1449 - accuracy: 0.9299\n",
            "Epoch 16/50\n",
            "1172/1172 - 1s - loss: 0.1425 - accuracy: 0.9293\n",
            "Epoch 17/50\n",
            "1172/1172 - 1s - loss: 0.1395 - accuracy: 0.9328\n",
            "Epoch 18/50\n",
            "1172/1172 - 1s - loss: 0.1361 - accuracy: 0.9349\n",
            "Epoch 19/50\n",
            "1172/1172 - 1s - loss: 0.1342 - accuracy: 0.9346\n",
            "Epoch 20/50\n",
            "1172/1172 - 1s - loss: 0.1317 - accuracy: 0.9364\n",
            "Epoch 21/50\n",
            "1172/1172 - 1s - loss: 0.1299 - accuracy: 0.9377\n",
            "Epoch 22/50\n",
            "1172/1172 - 1s - loss: 0.1278 - accuracy: 0.9394\n",
            "Epoch 23/50\n",
            "1172/1172 - 1s - loss: 0.1259 - accuracy: 0.9410\n",
            "Epoch 24/50\n",
            "1172/1172 - 1s - loss: 0.1236 - accuracy: 0.9419\n",
            "Epoch 25/50\n",
            "1172/1172 - 1s - loss: 0.1222 - accuracy: 0.9423\n",
            "Epoch 26/50\n",
            "1172/1172 - 1s - loss: 0.1215 - accuracy: 0.9439\n",
            "Epoch 27/50\n",
            "1172/1172 - 1s - loss: 0.1203 - accuracy: 0.9450\n",
            "Epoch 28/50\n",
            "1172/1172 - 1s - loss: 0.1190 - accuracy: 0.9459\n",
            "Epoch 29/50\n",
            "1172/1172 - 1s - loss: 0.1188 - accuracy: 0.9456\n",
            "Epoch 30/50\n",
            "1172/1172 - 1s - loss: 0.1165 - accuracy: 0.9466\n",
            "Epoch 31/50\n",
            "1172/1172 - 1s - loss: 0.1146 - accuracy: 0.9485\n",
            "Epoch 32/50\n",
            "1172/1172 - 1s - loss: 0.1144 - accuracy: 0.9489\n",
            "Epoch 33/50\n",
            "1172/1172 - 1s - loss: 0.1128 - accuracy: 0.9482\n",
            "Epoch 34/50\n",
            "1172/1172 - 1s - loss: 0.1113 - accuracy: 0.9484\n",
            "Epoch 35/50\n",
            "1172/1172 - 1s - loss: 0.1101 - accuracy: 0.9504\n",
            "Epoch 36/50\n",
            "1172/1172 - 1s - loss: 0.1086 - accuracy: 0.9506\n",
            "Epoch 37/50\n",
            "1172/1172 - 1s - loss: 0.1065 - accuracy: 0.9523\n",
            "Epoch 38/50\n",
            "1172/1172 - 1s - loss: 0.1047 - accuracy: 0.9521\n",
            "Epoch 39/50\n",
            "1172/1172 - 1s - loss: 0.1038 - accuracy: 0.9531\n",
            "Epoch 40/50\n",
            "1172/1172 - 1s - loss: 0.1026 - accuracy: 0.9548\n",
            "Epoch 41/50\n",
            "1172/1172 - 1s - loss: 0.1020 - accuracy: 0.9544\n",
            "Epoch 42/50\n",
            "1172/1172 - 1s - loss: 0.0990 - accuracy: 0.9557\n",
            "Epoch 43/50\n",
            "1172/1172 - 1s - loss: 0.0976 - accuracy: 0.9572\n",
            "Epoch 44/50\n",
            "1172/1172 - 1s - loss: 0.0993 - accuracy: 0.9552\n",
            "Epoch 45/50\n",
            "1172/1172 - 1s - loss: 0.0957 - accuracy: 0.9580\n",
            "Epoch 46/50\n",
            "1172/1172 - 1s - loss: 0.0945 - accuracy: 0.9574\n",
            "Epoch 47/50\n",
            "1172/1172 - 1s - loss: 0.0967 - accuracy: 0.9571\n",
            "Epoch 48/50\n",
            "1172/1172 - 1s - loss: 0.0930 - accuracy: 0.9582\n",
            "Epoch 49/50\n",
            "1172/1172 - 1s - loss: 0.0933 - accuracy: 0.9594\n",
            "Epoch 50/50\n",
            "1172/1172 - 1s - loss: 0.0908 - accuracy: 0.9590\n",
            "confusion matrix: on test data set\n",
            "[[6285  143]\n",
            " [ 381 5691]]\n",
            "errorRate 0.042\n",
            "N=500 : experiment_type: random_roads  road_input_type: vibration\n",
            "========== 3 out of 4 replications========\n",
            "shuffling 50000 entries\n",
            "[3926, 37607, 47533, 32542, 12779, 16974, 19723, 1748, 49675, 23464]\n",
            "made 500 samples with botsample 400 topsample 500\n",
            "maxfreq=     4.00 M=  2000.00 V=   16.66 C=60000.00\n",
            "Total samples 50000 good runs 251 bad runs 249\n",
            "2021-05-23 03:16:04\n",
            "Epoch 1/50\n",
            "1172/1172 - 2s - loss: 0.3535 - accuracy: 0.8383\n",
            "Epoch 2/50\n",
            "1172/1172 - 1s - loss: 0.3115 - accuracy: 0.8538\n",
            "Epoch 3/50\n",
            "1172/1172 - 1s - loss: 0.2934 - accuracy: 0.8595\n",
            "Epoch 4/50\n",
            "1172/1172 - 1s - loss: 0.2630 - accuracy: 0.8721\n",
            "Epoch 5/50\n",
            "1172/1172 - 1s - loss: 0.2334 - accuracy: 0.8906\n",
            "Epoch 6/50\n",
            "1172/1172 - 1s - loss: 0.2094 - accuracy: 0.8994\n",
            "Epoch 7/50\n",
            "1172/1172 - 1s - loss: 0.1918 - accuracy: 0.9046\n",
            "Epoch 8/50\n",
            "1172/1172 - 1s - loss: 0.1787 - accuracy: 0.9087\n",
            "Epoch 9/50\n",
            "1172/1172 - 1s - loss: 0.1670 - accuracy: 0.9135\n",
            "Epoch 10/50\n",
            "1172/1172 - 1s - loss: 0.1591 - accuracy: 0.9159\n",
            "Epoch 11/50\n",
            "1172/1172 - 1s - loss: 0.1527 - accuracy: 0.9193\n",
            "Epoch 12/50\n",
            "1172/1172 - 1s - loss: 0.1475 - accuracy: 0.9212\n",
            "Epoch 13/50\n",
            "1172/1172 - 1s - loss: 0.1434 - accuracy: 0.9235\n",
            "Epoch 14/50\n",
            "1172/1172 - 1s - loss: 0.1387 - accuracy: 0.9260\n",
            "Epoch 15/50\n",
            "1172/1172 - 1s - loss: 0.1361 - accuracy: 0.9260\n",
            "Epoch 16/50\n",
            "1172/1172 - 1s - loss: 0.1323 - accuracy: 0.9287\n",
            "Epoch 17/50\n",
            "1172/1172 - 1s - loss: 0.1284 - accuracy: 0.9322\n",
            "Epoch 18/50\n",
            "1172/1172 - 1s - loss: 0.1229 - accuracy: 0.9377\n",
            "Epoch 19/50\n",
            "1172/1172 - 1s - loss: 0.1164 - accuracy: 0.9415\n",
            "Epoch 20/50\n",
            "1172/1172 - 1s - loss: 0.1107 - accuracy: 0.9450\n",
            "Epoch 21/50\n",
            "1172/1172 - 1s - loss: 0.1036 - accuracy: 0.9503\n",
            "Epoch 22/50\n",
            "1172/1172 - 1s - loss: 0.0985 - accuracy: 0.9553\n",
            "Epoch 23/50\n",
            "1172/1172 - 1s - loss: 0.0936 - accuracy: 0.9577\n",
            "Epoch 24/50\n",
            "1172/1172 - 1s - loss: 0.0905 - accuracy: 0.9590\n",
            "Epoch 25/50\n",
            "1172/1172 - 1s - loss: 0.0865 - accuracy: 0.9610\n",
            "Epoch 26/50\n",
            "1172/1172 - 1s - loss: 0.0829 - accuracy: 0.9638\n",
            "Epoch 27/50\n",
            "1172/1172 - 1s - loss: 0.0796 - accuracy: 0.9641\n",
            "Epoch 28/50\n",
            "1172/1172 - 1s - loss: 0.0782 - accuracy: 0.9647\n",
            "Epoch 29/50\n",
            "1172/1172 - 1s - loss: 0.0751 - accuracy: 0.9658\n",
            "Epoch 30/50\n",
            "1172/1172 - 1s - loss: 0.0735 - accuracy: 0.9670\n",
            "Epoch 31/50\n",
            "1172/1172 - 1s - loss: 0.0707 - accuracy: 0.9684\n",
            "Epoch 32/50\n",
            "1172/1172 - 1s - loss: 0.0677 - accuracy: 0.9691\n",
            "Epoch 33/50\n",
            "1172/1172 - 1s - loss: 0.0668 - accuracy: 0.9700\n",
            "Epoch 34/50\n",
            "1172/1172 - 1s - loss: 0.0639 - accuracy: 0.9715\n",
            "Epoch 35/50\n",
            "1172/1172 - 1s - loss: 0.0608 - accuracy: 0.9725\n",
            "Epoch 36/50\n",
            "1172/1172 - 1s - loss: 0.0598 - accuracy: 0.9731\n",
            "Epoch 37/50\n",
            "1172/1172 - 1s - loss: 0.0579 - accuracy: 0.9746\n",
            "Epoch 38/50\n",
            "1172/1172 - 1s - loss: 0.0568 - accuracy: 0.9753\n",
            "Epoch 39/50\n",
            "1172/1172 - 1s - loss: 0.0549 - accuracy: 0.9755\n",
            "Epoch 40/50\n",
            "1172/1172 - 1s - loss: 0.0541 - accuracy: 0.9759\n",
            "Epoch 41/50\n",
            "1172/1172 - 1s - loss: 0.0511 - accuracy: 0.9779\n",
            "Epoch 42/50\n",
            "1172/1172 - 1s - loss: 0.0509 - accuracy: 0.9774\n",
            "Epoch 43/50\n",
            "1172/1172 - 1s - loss: 0.0484 - accuracy: 0.9792\n",
            "Epoch 44/50\n",
            "1172/1172 - 1s - loss: 0.0501 - accuracy: 0.9782\n",
            "Epoch 45/50\n",
            "1172/1172 - 1s - loss: 0.0463 - accuracy: 0.9798\n",
            "Epoch 46/50\n",
            "1172/1172 - 1s - loss: 0.0466 - accuracy: 0.9792\n",
            "Epoch 47/50\n",
            "1172/1172 - 1s - loss: 0.0459 - accuracy: 0.9801\n",
            "Epoch 48/50\n",
            "1172/1172 - 1s - loss: 0.0452 - accuracy: 0.9803\n",
            "Epoch 49/50\n",
            "1172/1172 - 1s - loss: 0.0430 - accuracy: 0.9809\n",
            "Epoch 50/50\n",
            "1172/1172 - 1s - loss: 0.0437 - accuracy: 0.9815\n",
            "confusion matrix: on test data set\n",
            "[[6092   66]\n",
            " [  96 6246]]\n",
            "errorRate 0.013\n",
            "N=500 : experiment_type: random_roads  road_input_type: vibration\n",
            "========== 4 out of 4 replications========\n",
            "The average error rate of 4 replications is:\n",
            "0.03952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEYhRq-ZWNxx"
      },
      "source": [
        "# Result Table\n",
        "\n",
        "Results for 3 models with different parameters (4 replications)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLJpnD2KXhmC"
      },
      "source": [
        "|ML algorithm|N runs|experiment type|observation type|error rates|result|\n",
        "|--- |--- |--- |--- |--- |--- |\n",
        "|KNN|100|random roads|road input|0.03400|success|\n",
        "|KNN|200|random roads|road input|0.02800|success|\n",
        "|KNN|200|one standard road|road input|0.00000|success|\n",
        "|KNN|200|random roads|in vehicle vibration|0.04550|success|\n",
        "|KNN|200|one standard road|in vehicle vibration|0.00000|success|\n",
        "|KNN|500|random roads|road input|0.01910|success|\n",
        "|Naive Bayes|200|random roads|road input|0.34888|fail|\n",
        "|Naive Bayes|200|one standard road|in vehicle vibration|0.00000|success|\n",
        "|Naive Bayes|200|random roads|in vehicle vibration|0.15037|success|\n",
        "|Naive Bayes*|200|one standard road|road input|0.31225|fail|\n",
        "|Naive Bayes|1000|random roads|road input|0.34304|fail|\n",
        "|MLP(20,5)|200|random roads|road input|0.00605|success|\n",
        "|MLP(20,5)|500|random roads|road input|0.00584|success|\n",
        "|MLP(20,5)|1000|random roads|road input|0.00419|success|\n",
        "|MLP(20,5)|500|one standard road|road input|0.00024|success|\n",
        "|MLP(20,5)|500|one standard road|in vehicle vibration|0.00000|success|\n",
        "|MLP(20,5)|500|random roads|in vehicle vibration|0.03952|success|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzNgtdssXikE"
      },
      "source": [
        "With the 20% threshold of an error rate, only the Naive Bayes model got failed results.\n",
        "\n",
        "### 1. K-Nearest Neighbour (KNN):\n",
        "KNN is a non-parametric algorithm. So it does not make any assumption on the data distribution. The main parameter it uses is the number of nearest neighbours. Objects are classified based on the most common class among the neighbours by their spacial distances (Similarity). \n",
        "\n",
        "In other words, **it has the advantage of easy implementation and it does not need to train some parameters**. Very effective to **noisy data (with random noise inputs from roads)**, and a big dataset (in this case we have 10k, 20k, 50k, 100k rows). \n",
        "\n",
        "However, it has the disadvantages that the k value needs to be determined by a human, and the computation cost would get much higher as the number of features gets higher.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 2. Naive Bayes Classifier\n",
        "This algorithm is naive because it assumes simply that every attribute is conditionally independent of each other.\n",
        "$$\n",
        "P(c|X) = P(x_1 | c) \\times P(x_2 | c) \\times \\cdots \\times P(x_n | c) \\times P(c)\n",
        "$$\n",
        "This simple assumption results in an extremely fast computation. **And it has the advantage of scalability when it comes to a large dataset, while other complicated models might take much more time exponentially**.\n",
        "\n",
        "However, Naive Bayes's simplicity leads to bad predictions. In this case, it is the only model that failed the requirement (< 20%). As it is shown on the table, when the observation type is not `in vehicle vibration`, the error rate will increase to over 30% regardless of the experiment type.\n",
        "\n",
        "**PS**: A new case was added (see the * line, **one standard road|road input**), and the result still failed. On the contrary, the KNN and MLP models got a very good accuracy at this case. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Multilayer Perceptron (MLP)\n",
        "As a basic form of artificial neural network (ANN), MLP can get a great result for classification problems. \n",
        "\n",
        "Neural networks are good to **model with nonlinear large data**. From the table, the error rate decreases from *0.00605 to 0.00419* when the number of runs increases from 200 to 1000. Although the training part takes a longer time as it needs to \"learn\" from the previous result and improves the weights for inputs, **it has the best result among these three models**. It also **predicts pretty fast** once it is trained.\n",
        "\n",
        "But neural networks **can only take numeric inputs**. Therefore, we need to transform the **label (y_rn) into binary values**. And it is hard to tune the model since the layers **are black boxes**. In addition, it requires a lot of guesswork since we **cannot see directly how much each independent variable is influencing others**. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da1de8LeGLpw"
      },
      "source": [
        "## Case Comparison\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sbZL2fBYgp7"
      },
      "source": [
        "From the result table, all models got a relatively high error rates in the case of **random roads** and **road input**. While the case of **standard road** and **in vehicle vibration** gave the lowest error rates among all combinations. \n",
        "\n",
        "### 1. standard road & in vehicle vibration (easiest)\n",
        "\n",
        "In the easiest case, there is **no random variable from the road and car damper**, which makes the model much easier to predict the car failure. As can be seen from the result, all three models **give 0.0% error rate** even for the worst model Naive Bayes. \n",
        "\n",
        "### 2. random roads & in vehicle vibration\n",
        "\n",
        "Once the **random road experiment** is introduced in the model, the result gets worse significantly since it has **one random variable from the road**. The model becomes a bit harder to determine whether the failure comes from the road condition or not.\n",
        "\n",
        "### 3. one standard road & road input\n",
        "\n",
        "When the road input is used, the result gets much worse compared to the second case. This introduces a variable to the model. **The vertical forces are generated by both the road and mass position response**. This results in the \"noise\" to the model, giving a higher error rate.\n",
        "\n",
        "### 4. random roads and road input (hardest)\n",
        "\n",
        "Combining the second and third cases, this case **introduces two variables to the model at the same time**, making the model much hard to predict the reason of failure.\n",
        "\n",
        "In conclusion, the more variables, the harder the prediction. But more runs (larger dataset) can lower the error rate to get a better model prediction of car failure. Besides, MLP gives the best result among them."
      ]
    }
  ]
}